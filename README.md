## 0727实验脚本


```
# 创建环境
conda create -n factory python=3.10 -y
conda activate factory

# 复制本仓库
git clone https://github.com/yang3121099/Shadow-FT-bk
mv Shadow-FT-bk Shadow
cd Shadow
pip install -e ".[torch,metrics]"
pip install importlib_metadata omegaconf
pip install torch==2.6.0 transformers==4.52.1 torchvision  deepspeed -U
cd ./opencompass
pip install -U opencompass
pip install -e .
export COMPASS_DATA_CACHE="." # 若报错则改为绝对路径的 "YOURS/opencompass/data"
wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip
python3 -m zipfile -e  OpenCompassData-core-20240207.zip  ./data

pip install lmdeploy evalplus==0.3.1 latex2sympy2_extended math_verify prettytable jieba rouge_chinese rank_bm25 gradio_client tree_sitter_languages  fuzzywuzzy  h5py
git clone github.com/open-compass/human-eval
cd human-eval && pip install -e .
pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git
```

# 生成训练代码

```
cd ../../../Shadow
bash run.sh

# 打开新创建的sh文件，全选粘贴到命令行即可
```
以下是Readme，师兄无需阅读


# Shadow-FT

Official repository for the paper **Shadow-FT: Tuning Instruct via Base**.

Shadow-FT fine‑tunes a *Base* language model with LoRA to obtain lightweight Δ (delta) parameters, then **merges** them onto its *Instruct* counterpart to boost instruction following. Training is powered by **LLaMA‑Factory**, evaluation by **OpenCompass**.

---

## Install

```bash
# Clone the repo
git clone https://github.com/wutaiqiang/Shadow-FT && cd Shadow

# Core dependencies (inherited from LLaMA‑Factory)
pip install -e ".[torch]" --no-build-isolation
```

---

## One‑click workflow

All operations are wrapped in **./run.sh**.  

---

## Sample `run.sh` log

```text
##### Auto-generated 2025-05-22 13:54:08 #####
# Model     : Qwen2.5-14B
# LoRA mode : true
# Template  : qwen

##### Environment #####
export VLLM_WORKER_MULTIPROC_METHOD=spawn

##### Training #####
###### I  max=2000  lr=1e-5 ######
llamafactory-cli train \
  --model_name_or_path "${MODEL_ROOT}/Qwen2.5-14B-Instruct" \
  --finetuning_type lora --lora_rank 128 \
  --dataset "Shadow_2k" \
  --output_dir "${OUTPUT_ROOT}/instruct_lora" ...

##### LoRA delta‑merge #####
llamafactory-cli export \
  --base_model "${MODEL_ROOT}/Qwen2.5-14B-Instruct" \
  --lora_dir   "${OUTPUT_ROOT}/delta" \
  --output_dir "${OUTPUT_ROOT}/shadow_instruct"

##### Evaluation list #####
# ('short_name', 'model_path')
```

*(The log is auto‑generated by ****\`\`****; edit the script to match your paths and GPUs.)*

---

## Cite

```bibtex
@article{shadowft2025,
  title  = {Shadow-FT: Tuning Instruct via Base},
  author = {Wu et al.},
  year   = {2025},
  eprint = {2505.12716},
  archivePrefix = {arXiv}
}
```

## License

Apache‑2.0.  Please also comply with the licenses of any upstream models and datasets.
