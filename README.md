# Shadow-FT

Official repository for the paper **Shadow-FT: Tuning Instruct via Base**.

Shadow-FT fine‑tunes a *Base* language model with LoRA to obtain lightweight Δ (delta) parameters, then **merges** them onto its *Instruct* counterpart to boost instruction following. Training is powered by **LLaMA‑Factory**, evaluation by **OpenCompass**.

---

## Install

```bash
# Clone the repo
git clone https://github.com/wutaiqiang/Shadow-FT && cd Shadow

# Core dependencies (inherited from LLaMA‑Factory)
pip install -e ".[torch]" --no-build-isolation
```

---

## One‑click workflow

All operations are wrapped in **./run.sh**.  

---

## Sample `run.sh` log

```text
##### Auto-generated 2025-05-22 13:54:08 #####
# Model     : Qwen2.5-14B
# LoRA mode : true
# Template  : qwen

##### Environment #####
export VLLM_WORKER_MULTIPROC_METHOD=spawn

##### Training #####
###### I  max=2000  lr=1e-5 ######
llamafactory-cli train \
  --model_name_or_path "${MODEL_ROOT}/Qwen2.5-14B-Instruct" \
  --finetuning_type lora --lora_rank 128 \
  --dataset "Shadow_2k" \
  --output_dir "${OUTPUT_ROOT}/instruct_lora" ...

##### LoRA delta‑merge #####
llamafactory-cli export \
  --base_model "${MODEL_ROOT}/Qwen2.5-14B-Instruct" \
  --lora_dir   "${OUTPUT_ROOT}/delta" \
  --output_dir "${OUTPUT_ROOT}/shadow_instruct"

##### Evaluation list #####
# ('short_name', 'model_path')
```

*(The log is auto‑generated by ****\`\`****; edit the script to match your paths and GPUs.)*

---

## Cite

```bibtex
@article{shadowft2025,
  title  = {Shadow-FT: Tuning Instruct via Base},
  author = {Wu et al.},
  year   = {2025},
  eprint = {2505.12716},
  archivePrefix = {arXiv}
}
```

## License

Apache‑2.0.  Please also comply with the licenses of any upstream models and datasets.
